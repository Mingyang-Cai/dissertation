\chapter{Introduction} 
\label{chap1}
	Missing data commonly occur in scientific research and have a significant impact on the downstream analysis. First, most statistical analyses require complete data and can not apply to incomplete data directly. Second, the reduced sample size arising from missing data generally induces estimates with less efficiency and results in a loss of statistical power \citep{RubinD1987}.

For instance, in clinical research, investigators may encounter difficulties with prognostic covariates measurements, such as estrogen receptor status, nodal status, or size of the tumour in cancer studies; CD4 counts in AIDS studies \citep{ibrahim2012missing}. Such missing variables restrict the assessment of balance in covariate distributions in observational studies. In social science, longitudinal studies are used to investigate some developmental trends over an extended period. However, longitudinal studies suffer attrition of participants. Missing data occur when participants drop out before the end of the study. In sample survey research, respondents may be assigned to a subset of all questions by design. Additionally, they may refuse to answer specific questions which involve private information (e.g., personal income) or cost much time (e.g., watching one movie before answering questions).  

Although missing data annoy scientists and researchers, the field of missing data analysis has been dramatically developed. A large number of approaches were proposed to handle missing data with different features. The general idea of missing data methods is to deduce information about missing data from what we observed. I will first introduce missingness mechanisms on which the properties of missing data methods depend and a comprehensive overview of missing data methods. Since only multiple imputation is discussed in this dissertation, I will also give details on the framework of multiple imputation based on the structure of this dissertation: how MI works, the software I used, data-driven and model-based methods, and diagnostics of imputation models.   

\section{Missingness mechanisms}
It is necessary to understand the reason for non-response since it helps us identify the missingness mechanism of the incomplete data and which missing data methods we could apply. \citet{rubin1976inference} assumed that the mechanisms of missing data could be modelled and defined three categories of missing data problems based on different missingness mechanisms. If the probability of variables being missing is equal for all cases, then the variables are missing completely at random (MCAR). An example of MCAR is when a random sample is drawn from the population, each individual has the same probability of being sampled. Individuals who are not included in the sample could be viewed as MCAR. In such a case, the observed data is still representative of the population, which implies the validity of complete data analysis. Although simplifying missing data problems, MCAR is often impractical. If the probability of variables being missing depends on the observed data, then the variables are missing at random (MAR), which is more general than MCAR. An example of MAR is that younger individuals have a higher probability of being sampled from the population, where the age of the collected sample is completely observed. Most missing data analyses are developed based on the MAR assumption because the MAR implies an ignorable missingness mechanism \citep{little2019statistical}. In such a case, the unobserved data share identical distributions as the observed data, which means that the model generated by the observed data could be applied to the missing data. However, it is notable that ignorable missingness mechanisms do not entirely disregard the missingness mechanisms. The imputer should consider variables that explain the probability of being missing. Sometimes, the missingness mechanisms are scientific interests. Suppose the probability of variables being missing depends on partially observed or unobserved data. Then, the variables are missing not at random (MNAR), which is a more complex situation to handle the missing data problem. MNAR means that the research has no information about the missingness mechanism. An example of MNAR is that a researcher would like to analyse the income of a population since, people with higher income tend to refuse income disclosure.  

The classification of missingness mechanisms inspires researchers to understand the reason for missing data occurrence and provide information about which missing data analyses will produce valid inferences. For instance, in general, mean imputation requires MCAR to be true.  

\section{Missing data methods}
There are mainly four categories of missing data approaches proposed in the literature: complete-case analysis, weighting procedures, direct maximum likelihood methods and multiple imputation methods \citep{little2019statistical}. The four methods are not mutually exclusive. This section will give an overview of the first three methods. Since this dissertation focuses on multiple imputation (MI), a more detailed introduction of MI will be given later. 

Complete case (CC) analysis, also named list-wise deletion, discards cases with missing values and only analyses complete observations. Although CC analysis is a straightforward and convenient solution to the missing data problem, it yields unbiased but less efficient mean vectors, covariance matrix, and regression weights merely under MCAR. Sometimes, it may also produce acceptable results when there are a minor amount of partially observed cases. However, CC analysis wastes a large amount of data, which leads to loss of precision and bias when missingness mechanisms are MAR or MNAR. 

Weighting methods commonly apply for unit non-response, which means that all outcome data for the respondent who refuses to participate in the survey are missing. The general idea of weighting analysis is analogous to weighting in randomisation inference. The weighting methods define the probability of selection ($\phi$) in the sample and apply the classical Horvitz - Thompson estimator \citep{horvitz1952generalization} to evaluate scientific interest. Suppose $y_i$ is the value of the variable $Y$ for unit $i$ ($i = 1, 2, \dots, n$), the Horvitz - Thompson estimator of the population mean is $\bar{Y}_{HT} = \sum_{i=1}^{n}\phi^{-1}y_i/n$. 

When there are non-responses units, the probability of response $\hat{p}$ should be considered and the adjusted estimator is:
\begin{equation*}
	\frac{\sum_{i=1}^{n}(\phi\hat{p}_i)^{-1}y_i}{\sum_{i=1}^{n}(\phi\hat{p}_i)^{-1}}
\end{equation*}

Weighting methods are a conceptually and computationally convenient procedure to reduce bias from CC analysis. However, although it takes account of missingness mechanisms, the application of weighing methods is limited because of no full use of incomplete observed units and less straightforward computation of variances. 

Direct maximum likelihood methods include extensive procedures that define a model for the complete data, estimate the parameters of the model with maximum likelihood and conduct statistical inferences. \citet{little2019statistical} developed sophisticated direct maximum likelihood methods to estimate parameters of likelihood, obtain standard errors from information matrix and create missing data. For example, suppose a random variable $Y$ consists of observed and missing parts ($Y_{obs}$ and $Y_{mis}$). Then, with the ignorable missingness mechanism assumption, the likelihood for the parameters of the imputation model ($\theta$) is an integral over the missing values:
\begin{equation*}
	L(\theta | Y_{obs}) =  \int f_{Y}(Y_{obs}, Y_{mis} | \theta)  \,dY_{mis}. 
\end{equation*}
Direct maximum likelihood methods could provide unbiased and efficient estimates under both MCAR and MAR. 

\section{Multiple imputation}
Multiple imputation (MI) is a general approach for the analysis of incomplete datasets. It involves generating several plausible imputed datasets and aggregating different results into a single inference. First, missing cells are filled with synthetic data drawn from corresponding posterior predictive distributions. Next, this procedure is repeated multiple times, resulting in several imputed datasets. The scientific interest is then estimated for each imputed dataset by complete-data analyses. Finally, different estimates are pooled into one inference using Rubinâ€™s rule, which accounts for within and across imputation uncertainty \citep{RubinD1987}.

Rubin proposed the main principles of MI at the end of the 70's. The uptake and development of MI techniques has been growing at a favorable rate over the last two decades. Nowadays, various technologies of MI are generated for different statistical models, for instance, multilevel multiple imputation \citep{longford2001multilevel}, MI for longitudinal data \citep{twisk2002attrition, demirtas2004modeling}, MI for structural equation modelling \citep{olinsky2003comparative, allison2003missing}. Because of the methodological development of MI, the techniques are applied to a wide range of fields (e.g., epidemiology \citep{mueller2008injuries}, politics \citep{tanasoiu2008determinants}, genetics \citep{souverein2006multiple}, psychology \citep{sundell2008transportability} and sociology \citep{finke2008cross}) and implemented in many software packages (e.g. \texttt{mice} and \texttt{mi} in \texttt{R}, \texttt{IVEWARE} in \texttt{SAS}, \texttt{ice} in \texttt{STATA} and module \texttt{MVA} in \texttt{SPSS}) \citep{Buuren2011, azur2011multiple}.  

There are two general approaches for imputing multivariate data: joint modelling (JM) and fully conditional specification (FCS). Joint modelling imputation assumes a model $p(Y^{mis}, Y^{obs}\,|\,\theta)$ for the complete data and a prior distribution $p(\theta)$ for the parameter $\theta$. Joint modelling partitions the observed data into groups based on the missing pattern and imputes the missing data within each missing pattern according to the corresponding predictive distribution. Under the assumption of ignorability, the parameters of the predictive distribution for different missing patterns are generated from the posterior joint distribution. \citet{schafer1997analysis} proposed joint modelling methods for multivariate normal data, categorical data and mixed normal-categorical data. Joint modelling approaches have solid theoretical properties (i.e., compatibility between the imputation and substantive models), while it lacks the flexibility of model specification.


Let $Y_{j}^{t} = (Y_{j}^{obs}, Y_{j}^{mis(t)}), j = 1, \dots, p$ denote the observed and imputed values of variable $Y_{j}$ at iteration $t$ and $Y_{-j}^{t} = (Y_{1}^{t}, \dots, Y_{j-1}^{t}, Y_{j+1}^{t-1}, \dots, Y_{p}^{t-1})$. Let $X$ denote fully observed variables in the dataset. Fully conditional specification specifies the distribution for each partially observed variable conditional on all other variables $P(Y_{j}|Y_{-j}, X, \theta_{j})$, where the vector $\theta_{j}$ is the coefficients of the fully condition model of $Y_{j}$, and imputes each missing variable iteratively. The FCS starts with naive imputations such as a random draw from the observed values. The \emph{t}th iteration for the incomplete variable $Y_{j}$ consists of the following draws:
\begin{align*}
	&\theta_{j}^{t} \sim f(\theta_{j})f(Y^{obs}_{j}|Y^{t}_{-j}, X, \theta_{j})\\
	&Y^{mis(t)}_{j} \sim f(Y^{mis}_{j}|Y^{t}_{j}, X, \theta_{j}^{t}),
\end{align*}
where $f(\theta_{j})$ is generally specified as a non-informative prior. After a sufficient number of iterations, typically with 5 to 10 iterations \citep{Buuren2018, oberman2020missing}, the stationary distribution is achieved. The final iteration generates a single imputed dataset and multiple imputations are created by applying FCS in parallel \emph{m} times. Since FCS provides tremendous flexibility in specifying imputation models for multivariate partially observed data, FCS is now a widely accepted and popular MI approach \citep{van2007multiple}. Even while, FCS lacks a satisfactory theory and has a potential risk in incompatibility. 

Hybrid imputation, also called block imputation, combines the flexibility of FCS with the attractive theoretical properties of JM \citep{van2018JSM}. A block consists of one or more variables. If the block has multiple variables, then multivariate imputation methods will be applied to impute those variables jointly. The joint modelling approach is the case where all variables form one block, while the FCS approach treats each variable as a separate block. 
When the imputation model of one variable is potentially incompatible, or when its theoretical properties are not thoroughly studied, hybrid imputation would merge that variable with other variables and apply the joint modelling imputation approach to that block. 

On the other hand, when the joint distribution of several missing variables is ambiguous, hybrid imputation could use the FCS approach to impute each variable. In general, the apparent advantage of hybrid imputation is the flexibility of model specification. However, hybrid imputation methods are hardly known or studied. 

\section{Aim}
This dissertation develops hybrid imputation in both methodological and practical aspects. Based on missingness patterns and restrictions on the data, different block-wise partition strategies and the corresponding block imputation methods are considered to provide plausible imputations. Although hybrid imputation is a broad and undeveloped field, it could be a more user-friendly and flexible strategy for multivariate imputation. This dissertation addresses the following hybrid imputation problems. 

First, in medical and epidemiological research, the analysis model commonly contains squared terms \citep{seaman2012multiple, bartlett2015multiple}. In order to generate plausible imputations, the MI procedure should preserve the relationship between the original variable and its squared counterpart. \citet{Vink2013} proposed a hot-deck multiple imputation method, named the polynomial combination (PC) method, for imputation models containing squared terms. The method yields unbiased regression estimates and preserves the quadratic relationships in the imputed data for both MCAR and MAR mechanisms. However, the coverage rate of the PC method is not thoroughly studied. Chapter \ref{chap2} dives into the coverage rate of the PC method and proposes a minor adjustment to the PC method. As a result, the performance of the PC method is improved to some extent.  

Second, in many domains of statistics, restrictions among variables are not limited to the quadratic effect, for instance, data transformations, interactions, and range restrictions. Therefore, it is sensible to embrace all relations of scientific interest in the imputation model. With hybrid imputation strategies, the researcher could set variables involved in a restriction into one block and perform a joint imputation. Chapter \ref{chap3} proposes a multivariate predictive mean matching (MPMM), which generalises univariate predictive mean matching \citep{rubin1986statistical, little1988missing} to impute multiple variables simultaneously. 

Third, causal inference is widely used in epidemiology, biology, and social science. The treatment effect is usually calculated by the average treatment effect. However, to provide more accurate treatment recommendations, the analyst should take account of the heterogeneity of treatment effects across individuals, also known as the individual treatment effect. Because only one of the potential outcomes is observed for each individual, the causal inference could also be viewed as a missing data problem. Chapter \ref{chap4} proposes a hybrid imputation method that sets potential outcomes in one block and imputes them to estimate individual treatment effects.

Fourth, the missing data and parameters of imputation models are often generated from the corresponding posterior distribution. However, the prior is limited to the non-informative setting. Multiple imputation with informative prior is not well developed. Open problems could be: how to implement MI with informative prior elegantly; the compatibility of FCS with informative prior; what is the corresponding priors of the substantive joint distribution when the compatibility of FCS with informative prior holds; How could MI with informative prior be applied in practice. Chapter \ref{chap5} investigates the compatibility of univariate imputation under normal linear regression with normal inverse-gamma priors.

Finally, a critical part of the multiple imputation process is selecting sensible models to generate plausible values for the missing data. The validity of complete data analysis on imputed datasets relies on the congeniality of the imputation model and the substantive model of interest \citep{meng1994multiple}. A comprehensive overview of model diagnostic in multiple imputation is available in \citet{nguyen2017model}. Chapter \ref{chap6} proposes a novel strategy to diagnose multiple imputation models based on posterior predictive checking. 

\section{Workflow of \texttt{MICE}}
A straightforward implementation of hybrid imputation can be found in the \texttt{MICE} algorithm proposed by \citet{Buuren2011}. Version 3.0 of \texttt{MICE} added a new block argument with which the user could partition the variables into blocks. The algorithm of \texttt{MICE} will iterate over blocks rather than variables. For the block that contains one variable, all imputation methods developed under the fully conditional specification framework are available. For the block that contains multiple variables, the \texttt{MICE} algorithm allows calling joint modelling imputation methods from other packages. For example, \textbf{mice::jomoImpute} is a wrapper around the \textbf{jomoImpute} function from the \texttt{mitml} package. This function is used for joint modelling imputation of multilevel data. 

The \texttt{MICE} package creates functions for three components: imputation, analysis, and pooling.  Imputed datasets are generated with function \textbf{mice()}. Complete data analysis are performed on every imputed dataset by \textbf{with()} function and combined into a single inference with function \textbf{pool()}. The software stores the output of each step in a particular class: \textbf{mids}, \textbf{mira} and \textbf{mipo}. 

The \texttt{MICE} algorithm starts with filling missing cells by randomly drawing values from the observed data. Subsequently, incomplete variables are imputed in a block-by-block iteration â€” a single iteration of the algorithm cycles through all specified blocks.

The number of iterations (argument \textbf{maxit} in \textbf{mice} function) and imputations (argument \textbf{m} in \textbf{mice} function) are of importance in MI. In general, a low number of iterations appear to be enough \citep{brand1999development, van1999multiple}. However, slow convergence may occur if there is a large amount of missing data or high autocorrelation between the imputation iterations. \citet{oberman2020missing} performed a simulation study and concluded that five to ten iterations are enough for inferential validity. The convergence of the \texttt{MICE} algorithm could be investigated by plotting the statistics of interest in each imputation chain. If no definite trends appear, convergence is achieved.  

The default number of imputations in \texttt{MICE} is set to be five. Although it may be beneficial to produce a higher number of imputed datasets \citep{royston2004multiple, graham2007many, bodner2008improves, white2011multiple}, \citet{schafer1998multiple} showed that in many cases, there is no remarkable advantage to pooling more imputed datasets. An immense number of imputations implies more data storage and computational intensity. However, it may not be a substantive issue with the development of computer hardware. 

\section{Model-based imputation}
The aim of model-based imputation is to find predictive models for incomplete variables. With the hybrid imputation strategy, both the univariate imputation model for one incomplete variable and the multivariate imputation model for a set of incomplete variables could be fitted based on how incomplete variables are partitioned. The model-based approaches have a solid underpinning for theory. Joint modelling procedures are commonly implemented by model-based imputation, for example, multivariate normal distribution for a set of incomplete continuous variables, multinomial distribution for a set of incomplete categorical variables and general location model for a set of incomplete mixed variables \citep{schafer1997analysis}. 

For fully conditional specification, standard model-based imputation procedures are available for substantive models based on regression models for continuous and discrete outcomes and the proportional hazards model. If necessary, the research could fit more complex parametric imputation models, such as hierarchical models and time series models. Since compatibility is a potential weakness of FCS, it is feasible to assess whether the iterative distribution of a set of univariate imputation models converges to a joint distribution with explicit imputation models. If not, the imputations may differ according to different orders in which missing variables are updated. The phenomenon is called the order effect. 

There are two strategies to set up the imputation models. The first one, explained by \citet{meng1994multiple}, is that assuming an imputation model, the researcher should assess whether the analysis model is congenial to the imputation model. The second one, proposed by \citet{bartlett2015multiple}, is that the researcher should start with the analysis model and then find an imputation model which is congenial to the analysis model. Both methods highlight the importance of the match between the substantive model and the imputation model. If there are solid scientific models for the incomplete data, model-based imputation will ensure the generated values are compatible with the substantive models. Otherwise, if a broad range of candidate models fit the data, or if there is no convinced substantive model, it will be challenging to find an imputation model to accommodate every substantive model. 

For example, to estimate individual causal effects, we assume a multivariate normal distribution for the continuous potential outcomes, the validity of which is illustrated by \citet{imbens2015causal}. In such a case, we generally partition potential outcomes and covariates into separate blocks. The continuous potential outcomes are drawn from a conditional joint normal distribution, while the incomplete covariates could be imputed with fully conditional models. 

\section{Data-based imputation}
The most widely used data-based imputation method is predictive mean matching (PMM), proposed by \citet{rubin1986statistical} and \citet{little1988missing}. Although there are other non-parametric imputation methods such as tree-based imputation methods (e.g., classification and regression trees and random forest), we mainly develop imputation methods based on PMM and overview it. 

PMM calculates the estimated value for the observed and unobserved part of an incomplete variable through Bayesian normal linear regression. First, the procedure selects a set of candidate donors from all complete cases by minimising the distance between the predicted value of the missing unit and the predicted value of all observed units. Then, the unobserved value is imputed by randomly drawing one of the observed values of the candidate donors \citep{Buuren2018}.

Predictive mean matching has been proven to perform well in a wide range of simulation studies and is an attractive way to impute missing data \citep{Buuren2011, Heitjan1991, Morris2014, Vink2014, Vink2015}. PMM is applicable to missing variables at all measurement levels. The imputations produced by PMM would always fall in the range of observed values, so they are realistic. If observations are strictly positive, so will the imputations from PMM be. When applying PMM, the researcher could avoid data transformations to accommodate the assumption of the parametric imputation model. For example, imputed with Bayesian normal linear regression, the missing outcome should be transformed to validate the homoscedastic and normal assumptions. Furthermore, there is no need to post-processing the imputed values to satisfy intrinsic restrictions in the data. However, since PMM is a univariate imputation method, it cannot ensure the relations among a set of variables. 

When there is no definite substantive model, data-based imputation is an excellent alternative to produce plausible imputations. However,non-parametric imputation methods are not a substitute for sloppy modelling. For example, when the procedure of drawing imputed values from the observed values is not reasonable, the derived imputed dataset can yield poor inference. Another potential issue for data-based imputation methods is that they cannot extrapolate beyond the range of data and may create implausible imputations if the data is sparsely observed in some space. 

In short, PMM and other kinds of data-based imputation methods are a proper choice if the researchers think the fitted parametric model is flawed, and the validity of inference will weaken drastically. The methods work better with large samples and provide imputations that capture many patterns of the complete data. 

\section{Imputation model evaluation}
Assessing the fitness of the imputation model is also important in the MI procedure. Poor specification of the imputation model may lead to inconsistent imputed values and invalid estimates of target statistics. \citet{nguyen2017model} introduced currently available approaches for imputation model diagnostics such as comparison between the imputed value and observed data, cross-validation techniques and posterior predictive checking of scientific interests. The \texttt{MICE} package contains several graphical functions, such as \textbf{bwplot()}, \textbf{stripplot()}, \textbf{densityplot()} and \textbf{xyplot()}, to produce stripplot, box and whiskers plot, density plot and point plot of observed and imputed data to identify whether distributional discrepancy between observed and imputed data appears. 

\section{Outline of the dissertation}
This dissertation reports the investigations on methodological and practical aspects of multiple imputation. There are three parts. Part one considers data-based imputation strategies to jointly impute a block of variables so that data restrictions can be preserved. More specifically, two methods discussed in part one are generalisations of PMM. Part two considers model-based imputation methods, which assume a multivariate normal distribution for a block of incomplete variables. The compatibility of the joint model and the corresponding univariate conditional distributions for informative priors is evaluated. Part three focuses on imputation model checking. The discussed model diagnostic approach works for both model-based and data-based imputation methods.  
